{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class onering_conv_layer(nn.Module):\n",
    "    \"\"\"The convolutional layer on icosahedron discretized sphere using \n",
    "    1-ring filter\n",
    "    \n",
    "    Parameters:\n",
    "            in_feats (int) - - input features/channels\n",
    "            out_feats (int) - - output features/channels\n",
    "            \n",
    "    Input: \n",
    "        N x in_feats tensor\n",
    "    Return:\n",
    "        N x out_feats tensor\n",
    "    \"\"\"  \n",
    "    def __init__(self, in_feats, out_feats, neigh_orders, neigh_indices=None, neigh_weights=None):\n",
    "        super(onering_conv_layer, self).__init__()\n",
    "\n",
    "        self.in_feats = in_feats\n",
    "        self.out_feats = out_feats\n",
    "        self.neigh_orders = neigh_orders\n",
    "        self.pool = pool_layer(self.neigh_orders, pooling_type='mean')\n",
    "        self.weight = nn.Linear(7 * in_feats, out_feats)\n",
    "        # self.norm = nn.BatchNorm1d(out_feats, momentum=0.15, affine=True, track_running_stats=False)\n",
    "        # self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # self.dropout = nn.Dropout(0.7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        mat = x[self.neigh_orders].view(len(x), 7*self.in_feats)\n",
    "                \n",
    "        out_features = self.weight(mat)\n",
    "        # out_features = self.norm(out_features)\n",
    "        # out_features = self.relu(out_features)\n",
    "        # out_features = self.pool(out_features)\n",
    "        # out_features = self.dropout(out_features)\n",
    "        \n",
    "        return out_features\n",
    "    \n",
    "    \n",
    "\n",
    "class pool_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    The pooling layer on icosahedron discretized sphere using 1-ring filter\n",
    "    \n",
    "    Input: \n",
    "        N x D tensor\n",
    "    Return:\n",
    "        ((N+6)/4) x D tensor\n",
    "    \n",
    "    \"\"\"  \n",
    "\n",
    "    def __init__(self, neigh_orders, pooling_type='mean'):\n",
    "        super(pool_layer, self).__init__()\n",
    "\n",
    "        self.neigh_orders = neigh_orders\n",
    "        self.pooling_type = pooling_type\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        num_nodes = int((x.size()[0]+6)/4)\n",
    "        feat_num = x.size()[1]\n",
    "        x = x[self.neigh_orders[0:num_nodes*7]].view(num_nodes, feat_num, 7)\n",
    "        if self.pooling_type == \"mean\":\n",
    "            x = torch.mean(x, 2)\n",
    "        if self.pooling_type == \"max\":\n",
    "            x = torch.max(x, 2)\n",
    "            assert(x[0].size() == torch.Size([num_nodes, feat_num]))\n",
    "            return x[0], x[1]\n",
    "        \n",
    "        assert(x.size() == torch.Size([num_nodes, feat_num]))\n",
    "                \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "def Get_neighs_order(rotated=0):\n",
    "    neigh_orders_163842 = get_neighs_order(163842, rotated)\n",
    "    neigh_orders_40962 = get_neighs_order(40962, rotated)\n",
    "    neigh_orders_10242 = get_neighs_order(10242, rotated)\n",
    "    neigh_orders_2562 = get_neighs_order(2562, rotated)\n",
    "    neigh_orders_642 = get_neighs_order(642, rotated)\n",
    "    neigh_orders_162 = get_neighs_order(162, rotated)\n",
    "    neigh_orders_42 = get_neighs_order(42, rotated)\n",
    "    neigh_orders_12 = get_neighs_order(12, rotated)\n",
    "    \n",
    "    return neigh_orders_163842, neigh_orders_40962, neigh_orders_10242,\\\n",
    "        neigh_orders_2562, neigh_orders_642, neigh_orders_162, neigh_orders_42, neigh_orders_12\n",
    "  \n",
    "def get_neighs_order(n_vertex, rotated=0):\n",
    "    adj_mat_order = sio.loadmat(abspath +'/neigh_indices/adj_mat_order_'+ \\\n",
    "                                str(n_vertex) +'_rotated_' + str(rotated) + '.mat')\n",
    "    adj_mat_order = adj_mat_order['adj_mat_order']\n",
    "    neigh_orders = np.zeros((len(adj_mat_order), 7))\n",
    "    neigh_orders[:,0:6] = adj_mat_order-1\n",
    "    neigh_orders[:,6] = np.arange(len(adj_mat_order))\n",
    "    neigh_orders = np.ravel(neigh_orders).astype(np.int64)\n",
    "    \n",
    "    return neigh_orders\n",
    "\n",
    "def Get_upconv_index(rotated=0):\n",
    "    \n",
    "    upconv_top_index_163842, upconv_down_index_163842 = get_upconv_index(abspath+'/neigh_indices/adj_mat_order_163842_rotated_' + str(rotated) + '.mat')\n",
    "    upconv_top_index_40962, upconv_down_index_40962 = get_upconv_index(abspath+'/neigh_indices/adj_mat_order_40962_rotated_' + str(rotated) + '.mat')\n",
    "    upconv_top_index_10242, upconv_down_index_10242 = get_upconv_index(abspath+'/neigh_indices/adj_mat_order_10242_rotated_' + str(rotated) + '.mat')\n",
    "    upconv_top_index_2562, upconv_down_index_2562 = get_upconv_index(abspath+'/neigh_indices/adj_mat_order_2562_rotated_' + str(rotated) + '.mat')\n",
    "    upconv_top_index_642, upconv_down_index_642 = get_upconv_index(abspath+'/neigh_indices/adj_mat_order_642_rotated_' + str(rotated) + '.mat')\n",
    "    upconv_top_index_162, upconv_down_index_162 = get_upconv_index(abspath+'/neigh_indices/adj_mat_order_162_rotated_' + str(rotated) + '.mat')\n",
    "    \n",
    "    #TODO: return tuples of each level\n",
    "    return upconv_top_index_163842, upconv_down_index_163842, upconv_top_index_40962, upconv_down_index_40962, upconv_top_index_10242, upconv_down_index_10242,  upconv_top_index_2562, upconv_down_index_2562,  upconv_top_index_642, upconv_down_index_642, upconv_top_index_162, upconv_down_index_162\n",
    "\n",
    "\n",
    "def get_upconv_index(order_path):  \n",
    "    adj_mat_order = sio.loadmat(order_path)\n",
    "    adj_mat_order = adj_mat_order['adj_mat_order']\n",
    "    adj_mat_order = adj_mat_order -1\n",
    "    nodes = len(adj_mat_order)\n",
    "    next_nodes = int((len(adj_mat_order)+6)/4)\n",
    "    upconv_top_index = np.zeros(next_nodes).astype(np.int64) - 1\n",
    "    for i in range(next_nodes):\n",
    "        upconv_top_index[i] = i * 7 + 6\n",
    "    upconv_down_index = np.zeros((nodes-next_nodes) * 2).astype(np.int64) - 1\n",
    "    for i in range(next_nodes, nodes):\n",
    "        raw_neigh_order = adj_mat_order[i]\n",
    "        parent_nodes = raw_neigh_order[raw_neigh_order < next_nodes]\n",
    "        assert(len(parent_nodes) == 2)\n",
    "        for j in range(2):\n",
    "            parent_neigh = adj_mat_order[parent_nodes[j]]\n",
    "            index = np.where(parent_neigh == i)[0][0]\n",
    "            upconv_down_index[(i-next_nodes)*2 + j] = parent_nodes[j] * 7 + index\n",
    "    \n",
    "    return upconv_top_index, upconv_down_index\n",
    "\n",
    "\n",
    "def get_upsample_order(n_vertex):\n",
    "    n_last = int((n_vertex+6)/4)\n",
    "    neigh_orders = get_neighs_order(abspath+'/neigh_indices/adj_mat_order_'+ str(n_vertex) +'_rotated_0.mat')\n",
    "    neigh_orders = neigh_orders.reshape(n_vertex, 7)\n",
    "    neigh_orders = neigh_orders[n_last:,:]\n",
    "    row, col = (neigh_orders < n_last).nonzero()\n",
    "    assert len(row) == (n_vertex - n_last)*2, \"len(row) == (n_vertex - n_last)*2, error!\"\n",
    "    \n",
    "    u, indices, counts = np.unique(row, return_index=True, return_counts=True)\n",
    "    assert len(u) == n_vertex - n_last, \"len(u) == n_vertex - n_last, error\"\n",
    "    assert u.min() == 0 and u.max() == n_vertex-n_last-1, \"u.min() == 0 and u.max() == n_vertex-n_last-1, error\"\n",
    "    assert (indices == np.asarray(list(range(n_vertex - n_last))) * 2).sum() == n_vertex - n_last, \"(indices == np.asarray(list(range(n_vertex - n_last))) * 2).sum() == n_vertex - n_last, error\"\n",
    "    assert (counts == 2).sum() == n_vertex - n_last, \"(counts == 2).sum() == n_vertex - n_last, error\"\n",
    "    \n",
    "    upsample_neighs_order = neigh_orders[row, col]\n",
    "    \n",
    "    return upsample_neighs_order\n",
    "\n",
    "\n",
    "abspath = 'C:/Users/DELL/Desktop/kaiti/SphericalUNetPackage/sphericalunet/utils'\n",
    "neigh_orders = Get_neighs_order()\n",
    "neigh_orders = neigh_orders[1:]\n",
    "a, b, upconv_top_index_40962, upconv_down_index_40962, upconv_top_index_10242, upconv_down_index_10242,  upconv_top_index_2562, upconv_down_index_2562,  upconv_top_index_642, upconv_down_index_642, upconv_top_index_162, upconv_down_index_162 = Get_upconv_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio \n",
    "import torch.nn as nn\n",
    "class down_block(nn.Module):\n",
    "    \"\"\"\n",
    "    downsampling block in spherical unet\n",
    "    mean pooling => (conv => BN => ReLU) * 2\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, conv_layer, in_ch, out_ch, neigh_orders, pool_neigh_orders, first = False):\n",
    "        super(down_block, self).__init__()\n",
    "\n",
    "\n",
    "#        Batch norm version\n",
    "        if first:\n",
    "            self.block = nn.Sequential(\n",
    "                conv_layer(in_ch, out_ch, neigh_orders),\n",
    "                nn.BatchNorm1d(out_ch, momentum=0.15, affine=True, track_running_stats=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                conv_layer(out_ch, out_ch, neigh_orders),\n",
    "                nn.BatchNorm1d(out_ch, momentum=0.15, affine=True, track_running_stats=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "            \n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                pool_layer(pool_neigh_orders, 'mean'),\n",
    "                conv_layer(in_ch, out_ch, neigh_orders),\n",
    "                nn.BatchNorm1d(out_ch, momentum=0.15, affine=True, track_running_stats=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                conv_layer(out_ch, out_ch, neigh_orders),\n",
    "                nn.BatchNorm1d(out_ch, momentum=0.15, affine=True, track_running_stats=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch norm version\n",
    "        x = self.block(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class upconv_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    The transposed convolution layer on icosahedron discretized sphere using 1-ring filter\n",
    "    \n",
    "    Input: \n",
    "        N x in_feats, tensor\n",
    "    Return:\n",
    "        ((Nx4)-6) x out_feats, tensor\n",
    "    \n",
    "    \"\"\"  \n",
    "\n",
    "    def __init__(self, in_feats, out_feats, upconv_top_index, upconv_down_index):\n",
    "        super(upconv_layer, self).__init__()\n",
    "\n",
    "        self.in_feats = in_feats\n",
    "        self.out_feats = out_feats\n",
    "        self.upconv_top_index = upconv_top_index\n",
    "        self.upconv_down_index = upconv_down_index\n",
    "        self.weight = nn.Linear(in_feats, 7 * out_feats)\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        raw_nodes = x.size()[0]\n",
    "        new_nodes = int(raw_nodes*4 - 6)\n",
    "        x = self.weight(x)\n",
    "        x = x.view(len(x) * 7, self.out_feats)\n",
    "        x1 = x[self.upconv_top_index]\n",
    "        assert(x1.size() == torch.Size([raw_nodes, self.out_feats]))\n",
    "        x2 = x[self.upconv_down_index].view(-1, self.out_feats, 2)\n",
    "        x = torch.cat((x1,torch.mean(x2, 2)), 0)\n",
    "        assert(x.size() == torch.Size([new_nodes, self.out_feats]))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class up_block(nn.Module):\n",
    "    \"\"\"Define the upsamping block in spherica uent\n",
    "    upconv => (conv => BN => ReLU) * 2\n",
    "    \n",
    "    Parameters:\n",
    "            in_ch (int) - - input features/channels\n",
    "            out_ch (int) - - output features/channels    \n",
    "            neigh_orders (tensor, int)  - - conv layer's filters' neighborhood orders\n",
    "            \n",
    "    \"\"\"    \n",
    "    def __init__(self, conv_layer, in_ch, out_ch, neigh_orders, upconv_top_index, upconv_down_index):\n",
    "        super(up_block, self).__init__()\n",
    "        \n",
    "        self.up = upconv_layer(in_ch, out_ch, upconv_top_index, upconv_down_index)\n",
    "        \n",
    "        # batch norm version\n",
    "        self.double_conv = nn.Sequential(\n",
    "             conv_layer(in_ch, out_ch, neigh_orders),\n",
    "             nn.BatchNorm1d(out_ch, momentum=0.15, affine=True, track_running_stats=False),\n",
    "             nn.LeakyReLU(0.2, inplace=True),\n",
    "             conv_layer(out_ch, out_ch, neigh_orders),\n",
    "             nn.BatchNorm1d(out_ch, momentum=0.15, affine=True, track_running_stats=False),\n",
    "             nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat((x1, x2), 1) \n",
    "        x = self.double_conv(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Unet_40k(nn.Module):\n",
    "    \"\"\"Define the Spherical UNet structure\n",
    "\n",
    "    \"\"\"    \n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        \"\"\" Initialize the Spherical UNet.\n",
    "\n",
    "        Parameters:\n",
    "            in_ch (int) - - input features/channels\n",
    "            out_ch (int) - - output features/channels\n",
    "        \"\"\"\n",
    "        super(Unet_40k, self).__init__()\n",
    "\n",
    "        #neigh_indices_10242, neigh_indices_2562, neigh_indices_642, neigh_indices_162, neigh_indices_42 = Get_indices_order()\n",
    "        #neigh_orders_10242, neigh_orders_2562, neigh_orders_642, neigh_orders_162, neigh_orders_42, neigh_orders_12 = Get_neighs_order()\n",
    "        \n",
    "        neigh_orders = Get_neighs_order()\n",
    "        neigh_orders = neigh_orders[1:]\n",
    "        a, b, upconv_top_index_40962, upconv_down_index_40962, upconv_top_index_10242, upconv_down_index_10242,  upconv_top_index_2562, upconv_down_index_2562,  upconv_top_index_642, upconv_down_index_642, upconv_top_index_162, upconv_down_index_162 = Get_upconv_index() \n",
    "\n",
    "        chs = [in_ch, 32, 64, 128, 256, 512]\n",
    "        \n",
    "        conv_layer = onering_conv_layer\n",
    "\n",
    "        self.down1 = down_block(conv_layer, chs[0], chs[1], neigh_orders[0], None, True)\n",
    "        self.down2 = down_block(conv_layer, chs[1], chs[2], neigh_orders[1], neigh_orders[0])\n",
    "        self.down3 = down_block(conv_layer, chs[2], chs[3], neigh_orders[2], neigh_orders[1])\n",
    "        self.down4 = down_block(conv_layer, chs[3], chs[4], neigh_orders[3], neigh_orders[2])\n",
    "        self.down5 = down_block(conv_layer, chs[4], chs[5], neigh_orders[4], neigh_orders[3])\n",
    "      \n",
    "        self.up1 = up_block(conv_layer, chs[5], chs[4], neigh_orders[3], upconv_top_index_642, upconv_down_index_642)\n",
    "        self.up2 = up_block(conv_layer, chs[4], chs[3], neigh_orders[2], upconv_top_index_2562, upconv_down_index_2562)\n",
    "        self.up3 = up_block(conv_layer, chs[3], chs[2], neigh_orders[1], upconv_top_index_10242, upconv_down_index_10242)\n",
    "        self.up4 = up_block(conv_layer, chs[2], chs[1], neigh_orders[0], upconv_top_index_40962, upconv_down_index_40962)\n",
    "        \n",
    "        self.outc = nn.Sequential(\n",
    "                nn.Linear(chs[1], out_ch)\n",
    "                )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x2 = self.down1(x)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x6 = self.down5(x5)\n",
    "        \n",
    "        x = self.up1(x6, x5)\n",
    "        x = self.up2(x, x4)\n",
    "        x = self.up3(x, x3)\n",
    "        x = self.up4(x, x2) # 40962 * 32\n",
    "        \n",
    "        x = self.outc(x) # 40962 * 36\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(504, 10242) (400, 10242)\n",
      "6721697 paramerters in total\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m model \u001b[38;5;241m=\u001b[39m Unet_40k(in_ch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, out_ch\u001b[38;5;241m=\u001b[39mout_channels)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m paramerters in total\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28msum\u001b[39m(x\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())))\n\u001b[1;32m---> 77\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m     79\u001b[0m val_criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\.conda\\envs\\graph\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\.conda\\envs\\graph\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\.conda\\envs\\graph\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 579 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\.conda\\envs\\graph\\lib\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\.conda\\envs\\graph\\lib\\site-packages\\torch\\nn\\modules\\module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\.conda\\envs\\graph\\lib\\site-packages\\torch\\nn\\modules\\module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "################################################################\n",
    "\"\"\" hyper-parameters \"\"\"\n",
    "# cuda = torch.device('cpu')\n",
    "device = torch.device(\"cuda:0\")\n",
    "batch_size = 1\n",
    "model_name = 'Unet_infant'  # 'Unet_infant', 'Unet_18', 'Unet_2ring', 'Unet_repa', 'fcn', 'SegNet', 'SegNet_max'\n",
    "up_layer = 'upsample_interpolation' # 'upsample_interpolation', 'upsample_fixindex' \n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "learning_rate = 0.001\n",
    "momentum = 0.99\n",
    "weight_decay = 0.0001\n",
    "fold = 1 # 1,2,3 \n",
    "################################################################\n",
    "\n",
    "data_file = 'C:/Users/DELL/Desktop/kaiti/SphericalUNet_ttl/data.npy'\n",
    "label_file = 'C:/Users/DELL/Desktop/kaiti/SphericalUNet_ttl/data.npy'\n",
    "\n",
    "# 从.npy文件中加载数据\n",
    "data_array = np.load(data_file)\n",
    "train_data = data_array[0:400,:]\n",
    "val_data = data_array[400:,:]\n",
    "print(data_array.shape,train_data.shape)\n",
    "label_array = np.load(label_file)\n",
    "train_label = label_array[0:400,:]\n",
    "val_label = label_array[400:,:]\n",
    "\n",
    "neighbors_path = 'C:/Users/DELL/Desktop/kaiti/Spherical_U-Net/neigh_indices/adj_mat_order_10242.mat'\n",
    "\n",
    "# def get_neighs_order(neighbors_path):\n",
    "#     adj_mat_order = sio.loadmat(neighbors_path)\n",
    "#     adj_mat_order = adj_mat_order['adj_mat_order']\n",
    "#     neigh_orders = np.zeros((len(adj_mat_order), 7))\n",
    "#     neigh_orders[:,0:6] = adj_mat_order-1\n",
    "#     neigh_orders[:,6] = np.arange(len(adj_mat_order))\n",
    "#     neigh_orders = np.ravel(neigh_orders).astype(np.int64)\n",
    "    \n",
    "#     return neigh_orders\n",
    "\n",
    "class BrainSphere(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.labels = label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'data': torch.from_numpy(self.data[idx]),\n",
    "            'label': torch.from_numpy(self.data[idx])\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = BrainSphere(train_data, train_label)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "\n",
    "val_dataset = BrainSphere(val_data, val_label)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "# for batch in train_dataloader:\n",
    "#     data, labels = batch['data'], batch['label']\n",
    "#     # 在这里可以进行你的训练或其他操作\n",
    "#     print(f\"Batch Data Shape: {data.shape}, Batch Label Shape: {labels.shape}\")\n",
    "\n",
    "model = Unet_40k(in_ch=1, out_ch=out_channels)\n",
    "\n",
    "print(\"{} paramerters in total\".format(sum(x.numel() for x in model.parameters())))\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "val_criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler1 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)\n",
    "\n",
    "\n",
    "def train_step(data, target):\n",
    "    # model.train()\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    prediction = model(data)\n",
    "    # print(prediction)\n",
    "    loss = criterion(prediction, target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(),prediction\n",
    "\n",
    "\n",
    "# # train_dice = [0, 0, 0, 0, 0]\n",
    "print('length of dataloader:', len(train_dataloader))\n",
    "loss_list = []\n",
    "val_loss_list = []\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    pre_list = []\n",
    "    label_list = []\n",
    "    model.train() \n",
    "\n",
    "    if epoch % 20 == 9:\n",
    "        scheduler1.step()\n",
    "    \n",
    "    for batch_idx, train_data in enumerate(train_dataloader):\n",
    "        data = train_data['data']\n",
    "        data = torch.squeeze(data).unsqueeze(1)\n",
    "        target = train_data['label']\n",
    "        target = torch.tensor(target, dtype=torch.float32)\n",
    "        # print(target)\n",
    "        loss,pre = train_step(data, target)\n",
    "        pre_list.append(pre.item())\n",
    "        label_list.append(target.item())\n",
    "        total_loss = total_loss + loss\n",
    "    # print(total_loss/len(train_dataloader))\n",
    "    loss_list.append(total_loss/len(train_dataloader))\n",
    "    # print(pre_list)\n",
    "    # print(label_list)\n",
    "\n",
    "    # model.eval()  # 切换到评估模式\n",
    "        \n",
    "    # with torch.no_grad():  # 禁用梯度计算，因为在验证阶段我们不需要反向传播\n",
    "    #     val_loss_total = 0\n",
    "    #     val_pre_list = []\n",
    "    #     val_label_list = []\n",
    "        \n",
    "    #     for batch_idy, val_dataset in enumerate(val_dataloader):\n",
    "    #         val_data = val_dataset['data']\n",
    "    #         val_data = torch.squeeze(val_data).unsqueeze(1)\n",
    "    #         val_target = val_dataset['label']\n",
    "    #         val_target = torch.tensor(val_target, dtype=torch.float32)\n",
    "    #         data, target = val_data.to(device), val_target.to(device)\n",
    "\n",
    "    #         prediction = model(data)\n",
    "    #         # print(prediction)\n",
    "    #         val_loss = val_criterion(prediction, target)\n",
    "    #         val_loss = val_loss.item()\n",
    "            \n",
    "\n",
    "\n",
    "    #         val_pre_list.append(prediction.item())\n",
    "    #         val_label_list.append(val_target.item())\n",
    "    #         val_loss_total = val_loss_total + val_loss\n",
    "        \n",
    "    #     # 计算平均验证损失\n",
    "    #     avg_val_loss = val_loss_total / len(val_dataloader)\n",
    "    #     val_loss_list.append(val_loss_total / len(val_dataloader))\n",
    "    #     print(avg_val_loss)\n",
    "    #     correlation_coefficient = np.corrcoef(val_pre_list, val_label_list)[0, 1]\n",
    "    #     print(f'Correlation Coefficient: {correlation_coefficient}')\n",
    "        # print(val_pre_list)\n",
    "        # print(val_label_list)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(loss_list)\n",
    "# val_loss_list2 = []\n",
    "# for i in val_loss_list:\n",
    "#     val_loss_list2.append(i.item())\n",
    "\n",
    "plt.plot(val_loss_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
