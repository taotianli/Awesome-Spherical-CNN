{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from vit_pytorch.vit import Transformer, Attention, FeedForward, PreNorm\n",
    "\n",
    "class SiT(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                        dim, \n",
    "                        depth,\n",
    "                        heads,\n",
    "                        mlp_dim,\n",
    "                        pool = 'cls', \n",
    "                        num_patches = 20,\n",
    "                        num_classes= 1,\n",
    "                        num_channels =4,\n",
    "                        num_vertices = 2145,\n",
    "                        dim_head = 64,\n",
    "                        dropout = 0.,\n",
    "                        emb_dropout = 0.\n",
    "                        ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        patch_dim = num_channels * num_vertices\n",
    "\n",
    "        # inputs has size = b * c * n * v\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c n v  -> b n (v c)'),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        \n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "\n",
    "        return self.mlp_head(x)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
