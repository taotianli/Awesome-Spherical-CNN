{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from vit_pytorch.vit import Transformer, Attention, FeedForward, PreNorm\n",
    "\n",
    "class SiT(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                        dim, \n",
    "                        depth,\n",
    "                        heads,\n",
    "                        mlp_dim,\n",
    "                        pool = 'cls', \n",
    "                        num_patches = 20,\n",
    "                        num_classes= 1,\n",
    "                        num_channels =4,\n",
    "                        num_vertices = 2145,\n",
    "                        dim_head = 64,\n",
    "                        dropout = 0.,\n",
    "                        emb_dropout = 0.\n",
    "                        ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        assert pool in {'cls', 'mean', 'max', 'sum'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        patch_dim = num_channels * num_vertices\n",
    "\n",
    "        # inputs has size = b * c * n * v\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c n v  -> b n (v c)'),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        \n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_layer_swin(nn.Module):\n",
    "    \"\"\"The self-attention layer on icosahedron discretized sphere based on\n",
    "    2-ring filter\n",
    "    \n",
    "    Parameters:\n",
    "            in_feats (int) - - input features/channels\n",
    "            out_feats (int) - - output features/channels\n",
    "            num_heads (int) - - Number of attention heads\n",
    "            qkv_bias （bool） - - If True, add a learnable bias to query, key, value. Default: True\n",
    "            qk_scale (float) - - Override default qk scale of head_dim ** -0.5 if set\n",
    "            neigh_orders (ndarray) - - The indices of vertices used for patch partitioning\n",
    "    Input: \n",
    "        B x in_feats x N tensor \n",
    "    Return:\n",
    "        B x out_feats x N tensor\n",
    "    \"\"\"  \n",
    "    def __init__(self, in_feats, out_feats, neigh_orders, neigh_orders_2=None, head_dim=8,\n",
    "        qkv_bias=True, qk_scale=None, sep_process=True, drop_rate=None):\n",
    "        super(self_attention_layer_swin, self).__init__()\n",
    "\n",
    "        self.in_feats = in_feats\n",
    "        self.out_feats = out_feats\n",
    "\n",
    "        self.top = 16\n",
    "        self.down = 19\n",
    "\n",
    "        self.neigh_orders_top = neigh_orders['top'].reshape((-1, self.top))\n",
    "        self.neigh_orders_down = neigh_orders['down'].reshape((-1, self.down))\n",
    "        self.reverse_matrix = neigh_orders['reverse']\n",
    "        self.cnt_matrix = nn.parameter.Parameter(torch.from_numpy((1 / neigh_orders['count']).astype(np.float32)), requires_grad=False)\n",
    "\n",
    "        self.padding = nn.ZeroPad2d((0, 0, 0, 1))\n",
    "        \n",
    "        self.num_heads = in_feats // head_dim\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.sep_process = sep_process\n",
    "\n",
    "        if drop_rate:\n",
    "            self.drop_rate = drop_rate\n",
    "        else:\n",
    "            self.drop_rate = 0.0\n",
    "\n",
    "\n",
    "        self.qkv = nn.Linear(in_feats, in_feats * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_feats, out_feats),\n",
    "            nn.Dropout(p=self.drop_rate, inplace=True)\n",
    "            )\n",
    "        self.residual = nn.Linear(in_feats, out_feats)\n",
    "        \n",
    "        mlp_ratio = 2.00\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(out_feats, int(out_feats * mlp_ratio)),\n",
    "            nn.Dropout(p=self.drop_rate, inplace=True),\n",
    "            nn.Linear(int(out_feats * mlp_ratio), out_feats),\n",
    "            nn.Dropout(p=self.drop_rate, inplace=True)\n",
    "        )\n",
    "        self.norm = nn.BatchNorm1d(out_feats, momentum=0.15, affine=True, track_running_stats=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.Tensor.permute(x, (0, 2, 1))\n",
    "        res = self.residual(x)\n",
    "\n",
    "        B, N, C = x.shape  # batch size x number of vertices x channel\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        if self.sep_process:\n",
    "            attn_5adj = torch.einsum(\"binjc,binkc->binjk\", q[:, :, self.neigh_orders_top], k[:, :, self.neigh_orders_top])\n",
    "            attn_5adj = self.softmax(attn_5adj)\n",
    "\n",
    "            v_5adj = v[:, :, self.neigh_orders_top]\n",
    "            x_5adj = torch.einsum(\"binjk,binkc->binjc\", attn_5adj, v_5adj)\n",
    "\n",
    "            attn_6adj = torch.einsum(\"binjc,binkc->binjk\", q[:, :, self.neigh_orders_down], k[:, :, self.neigh_orders_down])\n",
    "            attn_6adj = self.softmax(attn_6adj)\n",
    "\n",
    "            v_6adj = v[:, :, self.neigh_orders_down]\n",
    "            x_6adj = torch.einsum(\"binjk,binkc->binjc\", attn_6adj, v_6adj)\n",
    "\n",
    "            x = torch.cat((x_5adj.reshape(B, self.num_heads, -1, C // self.num_heads), x_6adj.reshape(B, self.num_heads, -1, C // self.num_heads)), dim=2)\n",
    "            x = self.padding(x)\n",
    "            x = x[:, :, self.reverse_matrix, :].permute((0, 1, 3, 4, 2)) * self.cnt_matrix\n",
    "            x = torch.Tensor.sum(x.permute((0, 1, 4, 2, 3)), dim=3)\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).reshape(B, N, -1)\n",
    "\n",
    "        out_features = self.proj(x) + res\n",
    "        res2 = self.mlp(out_features)\n",
    "        out_features = torch.Tensor.permute(out_features, (0, 2, 1))\n",
    "        res2 = torch.Tensor.permute(res2, (0, 2, 1))\n",
    "        out_features = out_features + self.norm(res2)\n",
    "\n",
    "        return out_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
