{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class onering_conv_layer(nn.Module):\n",
    "    \"\"\"The convolutional layer on icosahedron discretized sphere using \n",
    "    1-ring filter\n",
    "    \n",
    "    Parameters:\n",
    "            in_feats (int) - - input features/channels\n",
    "            out_feats (int) - - output features/channels\n",
    "            \n",
    "    Input: \n",
    "        N x in_feats tensor\n",
    "    Return:\n",
    "        N x out_feats tensor\n",
    "    \"\"\"  \n",
    "    def __init__(self, in_feats, out_feats, neigh_orders, neigh_indices=None, neigh_weights=None):\n",
    "        super(onering_conv_layer, self).__init__()\n",
    "\n",
    "        self.in_feats = in_feats\n",
    "        self.out_feats = out_feats\n",
    "        self.neigh_orders = neigh_orders\n",
    "        self.pool = pool_layer(self.neigh_orders, pooling_type='mean')\n",
    "        self.weight = nn.Linear(7 * in_feats, out_feats)\n",
    "        # self.norm = nn.BatchNorm1d(out_feats, momentum=0.15, affine=True, track_running_stats=False)\n",
    "        # self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # self.dropout = nn.Dropout(0.7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        mat = x[self.neigh_orders].view(len(x), 7*self.in_feats)\n",
    "                \n",
    "        out_features = self.weight(mat)\n",
    "        # out_features = self.norm(out_features)\n",
    "        # out_features = self.relu(out_features)\n",
    "        # out_features = self.pool(out_features)\n",
    "        # out_features = self.dropout(out_features)\n",
    "        \n",
    "        return out_features\n",
    "    \n",
    "    \n",
    "\n",
    "class pool_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    The pooling layer on icosahedron discretized sphere using 1-ring filter\n",
    "    \n",
    "    Input: \n",
    "        N x D tensor\n",
    "    Return:\n",
    "        ((N+6)/4) x D tensor\n",
    "    \n",
    "    \"\"\"  \n",
    "\n",
    "    def __init__(self, neigh_orders, pooling_type='mean'):\n",
    "        super(pool_layer, self).__init__()\n",
    "\n",
    "        self.neigh_orders = neigh_orders\n",
    "        self.pooling_type = pooling_type\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        num_nodes = int((x.size()[0]+6)/4)\n",
    "        feat_num = x.size()[1]\n",
    "        x = x[self.neigh_orders[0:num_nodes*7]].view(num_nodes, feat_num, 7)\n",
    "        if self.pooling_type == \"mean\":\n",
    "            x = torch.mean(x, 2)\n",
    "        if self.pooling_type == \"max\":\n",
    "            x = torch.max(x, 2)\n",
    "            assert(x[0].size() == torch.Size([num_nodes, feat_num]))\n",
    "            return x[0], x[1]\n",
    "        \n",
    "        assert(x.size() == torch.Size([num_nodes, feat_num]))\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "def Get_neighs_order(rotated=0):\n",
    "    neigh_orders_163842 = get_neighs_order(163842, rotated)\n",
    "    neigh_orders_40962 = get_neighs_order(40962, rotated)\n",
    "    neigh_orders_10242 = get_neighs_order(10242, rotated)\n",
    "    neigh_orders_2562 = get_neighs_order(2562, rotated)\n",
    "    neigh_orders_642 = get_neighs_order(642, rotated)\n",
    "    neigh_orders_162 = get_neighs_order(162, rotated)\n",
    "    neigh_orders_42 = get_neighs_order(42, rotated)\n",
    "    neigh_orders_12 = get_neighs_order(12, rotated)\n",
    "    \n",
    "    return neigh_orders_163842, neigh_orders_40962, neigh_orders_10242,\\\n",
    "        neigh_orders_2562, neigh_orders_642, neigh_orders_162, neigh_orders_42, neigh_orders_12\n",
    "  \n",
    "def get_neighs_order(n_vertex, rotated=0):\n",
    "    adj_mat_order = sio.loadmat(abspath +'/neigh_indices/adj_mat_order_'+ \\\n",
    "                                str(n_vertex) +'_rotated_' + str(rotated) + '.mat')\n",
    "    adj_mat_order = adj_mat_order['adj_mat_order']\n",
    "    neigh_orders = np.zeros((len(adj_mat_order), 7))\n",
    "    neigh_orders[:,0:6] = adj_mat_order-1\n",
    "    neigh_orders[:,6] = np.arange(len(adj_mat_order))\n",
    "    neigh_orders = np.ravel(neigh_orders).astype(np.int64)\n",
    "    \n",
    "    return neigh_orders\n",
    "\n",
    "def Get_upconv_index(rotated=0):\n",
    "    \n",
    "    upconv_top_index_163842, upconv_down_index_163842 = get_upconv_index(abspath+'/neigh_indices/adj_mat_order_163842_rotated_' + str(rotated) + '.mat')\n",
    "    upconv_top_index_40962, upconv_down_index_40962 = get_upconv_index(abspath+'/neigh_indices/adj_mat_order_40962_rotated_' + str(rotated) + '.mat')\n",
    "    upconv_top_index_10242, upconv_down_index_10242 = get_upconv_index(abspath+'/neigh_indices/adj_mat_order_10242_rotated_' + str(rotated) + '.mat')\n",
    "    upconv_top_index_2562, upconv_down_index_2562 = get_upconv_index(abspath+'/neigh_indices/adj_mat_order_2562_rotated_' + str(rotated) + '.mat')\n",
    "    upconv_top_index_642, upconv_down_index_642 = get_upconv_index(abspath+'/neigh_indices/adj_mat_order_642_rotated_' + str(rotated) + '.mat')\n",
    "    upconv_top_index_162, upconv_down_index_162 = get_upconv_index(abspath+'/neigh_indices/adj_mat_order_162_rotated_' + str(rotated) + '.mat')\n",
    "    \n",
    "    #TODO: return tuples of each level\n",
    "    return upconv_top_index_163842, upconv_down_index_163842, upconv_top_index_40962, upconv_down_index_40962, upconv_top_index_10242, upconv_down_index_10242,  upconv_top_index_2562, upconv_down_index_2562,  upconv_top_index_642, upconv_down_index_642, upconv_top_index_162, upconv_down_index_162\n",
    "\n",
    "\n",
    "def get_upconv_index(order_path):  \n",
    "    adj_mat_order = sio.loadmat(order_path)\n",
    "    adj_mat_order = adj_mat_order['adj_mat_order']\n",
    "    adj_mat_order = adj_mat_order -1\n",
    "    nodes = len(adj_mat_order)\n",
    "    next_nodes = int((len(adj_mat_order)+6)/4)\n",
    "    upconv_top_index = np.zeros(next_nodes).astype(np.int64) - 1\n",
    "    for i in range(next_nodes):\n",
    "        upconv_top_index[i] = i * 7 + 6\n",
    "    upconv_down_index = np.zeros((nodes-next_nodes) * 2).astype(np.int64) - 1\n",
    "    for i in range(next_nodes, nodes):\n",
    "        raw_neigh_order = adj_mat_order[i]\n",
    "        parent_nodes = raw_neigh_order[raw_neigh_order < next_nodes]\n",
    "        assert(len(parent_nodes) == 2)\n",
    "        for j in range(2):\n",
    "            parent_neigh = adj_mat_order[parent_nodes[j]]\n",
    "            index = np.where(parent_neigh == i)[0][0]\n",
    "            upconv_down_index[(i-next_nodes)*2 + j] = parent_nodes[j] * 7 + index\n",
    "    \n",
    "    return upconv_top_index, upconv_down_index\n",
    "\n",
    "\n",
    "def get_upsample_order(n_vertex):\n",
    "    n_last = int((n_vertex+6)/4)\n",
    "    neigh_orders = get_neighs_order(abspath+'/neigh_indices/adj_mat_order_'+ str(n_vertex) +'_rotated_0.mat')\n",
    "    neigh_orders = neigh_orders.reshape(n_vertex, 7)\n",
    "    neigh_orders = neigh_orders[n_last:,:]\n",
    "    row, col = (neigh_orders < n_last).nonzero()\n",
    "    assert len(row) == (n_vertex - n_last)*2, \"len(row) == (n_vertex - n_last)*2, error!\"\n",
    "    \n",
    "    u, indices, counts = np.unique(row, return_index=True, return_counts=True)\n",
    "    assert len(u) == n_vertex - n_last, \"len(u) == n_vertex - n_last, error\"\n",
    "    assert u.min() == 0 and u.max() == n_vertex-n_last-1, \"u.min() == 0 and u.max() == n_vertex-n_last-1, error\"\n",
    "    assert (indices == np.asarray(list(range(n_vertex - n_last))) * 2).sum() == n_vertex - n_last, \"(indices == np.asarray(list(range(n_vertex - n_last))) * 2).sum() == n_vertex - n_last, error\"\n",
    "    assert (counts == 2).sum() == n_vertex - n_last, \"(counts == 2).sum() == n_vertex - n_last, error\"\n",
    "    \n",
    "    upsample_neighs_order = neigh_orders[row, col]\n",
    "    \n",
    "    return upsample_neighs_order\n",
    "\n",
    "\n",
    "abspath = 'C:/Users/DELL/Desktop/kaiti/SphericalUNetPackage/sphericalunet/utils'\n",
    "neigh_orders = Get_neighs_order()\n",
    "neigh_orders = neigh_orders[1:]\n",
    "a, b, upconv_top_index_40962, upconv_down_index_40962, upconv_top_index_10242, upconv_down_index_10242,  upconv_top_index_2562, upconv_down_index_2562,  upconv_top_index_642, upconv_down_index_642, upconv_top_index_162, upconv_down_index_162 = Get_upconv_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio \n",
    "import torch.nn as nn\n",
    "class down_block(nn.Module):\n",
    "    \"\"\"\n",
    "    downsampling block in spherical unet\n",
    "    mean pooling => (conv => BN => ReLU) * 2\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, conv_layer, in_ch, out_ch, neigh_orders, pool_neigh_orders, first = False):\n",
    "        super(down_block, self).__init__()\n",
    "\n",
    "\n",
    "#        Batch norm version\n",
    "        if first:\n",
    "            self.block = nn.Sequential(\n",
    "                conv_layer(in_ch, out_ch, neigh_orders),\n",
    "                nn.BatchNorm1d(out_ch, momentum=0.15, affine=True, track_running_stats=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                conv_layer(out_ch, out_ch, neigh_orders),\n",
    "                nn.BatchNorm1d(out_ch, momentum=0.15, affine=True, track_running_stats=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "            \n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                pool_layer(pool_neigh_orders, 'mean'),\n",
    "                conv_layer(in_ch, out_ch, neigh_orders),\n",
    "                nn.BatchNorm1d(out_ch, momentum=0.15, affine=True, track_running_stats=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                conv_layer(out_ch, out_ch, neigh_orders),\n",
    "                nn.BatchNorm1d(out_ch, momentum=0.15, affine=True, track_running_stats=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch norm version\n",
    "        x = self.block(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class upconv_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    The transposed convolution layer on icosahedron discretized sphere using 1-ring filter\n",
    "    \n",
    "    Input: \n",
    "        N x in_feats, tensor\n",
    "    Return:\n",
    "        ((Nx4)-6) x out_feats, tensor\n",
    "    \n",
    "    \"\"\"  \n",
    "\n",
    "    def __init__(self, in_feats, out_feats, upconv_top_index, upconv_down_index):\n",
    "        super(upconv_layer, self).__init__()\n",
    "\n",
    "        self.in_feats = in_feats\n",
    "        self.out_feats = out_feats\n",
    "        self.upconv_top_index = upconv_top_index\n",
    "        self.upconv_down_index = upconv_down_index\n",
    "        self.weight = nn.Linear(in_feats, 7 * out_feats)\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        raw_nodes = x.size()[0]\n",
    "        new_nodes = int(raw_nodes*4 - 6)\n",
    "        x = self.weight(x)\n",
    "        x = x.view(len(x) * 7, self.out_feats)\n",
    "        x1 = x[self.upconv_top_index]\n",
    "        assert(x1.size() == torch.Size([raw_nodes, self.out_feats]))\n",
    "        x2 = x[self.upconv_down_index].view(-1, self.out_feats, 2)\n",
    "        x = torch.cat((x1,torch.mean(x2, 2)), 0)\n",
    "        assert(x.size() == torch.Size([new_nodes, self.out_feats]))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class up_block(nn.Module):\n",
    "    \"\"\"Define the upsamping block in spherica uent\n",
    "    upconv => (conv => BN => ReLU) * 2\n",
    "    \n",
    "    Parameters:\n",
    "            in_ch (int) - - input features/channels\n",
    "            out_ch (int) - - output features/channels    \n",
    "            neigh_orders (tensor, int)  - - conv layer's filters' neighborhood orders\n",
    "            \n",
    "    \"\"\"    \n",
    "    def __init__(self, conv_layer, in_ch, out_ch, neigh_orders, upconv_top_index, upconv_down_index):\n",
    "        super(up_block, self).__init__()\n",
    "        \n",
    "        self.up = upconv_layer(in_ch, out_ch, upconv_top_index, upconv_down_index)\n",
    "        \n",
    "        # batch norm version\n",
    "        self.double_conv = nn.Sequential(\n",
    "             conv_layer(in_ch, out_ch, neigh_orders),\n",
    "             nn.BatchNorm1d(out_ch, momentum=0.15, affine=True, track_running_stats=False),\n",
    "             nn.LeakyReLU(0.2, inplace=True),\n",
    "             conv_layer(out_ch, out_ch, neigh_orders),\n",
    "             nn.BatchNorm1d(out_ch, momentum=0.15, affine=True, track_running_stats=False),\n",
    "             nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat((x1, x2), 1) \n",
    "        x = self.double_conv(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Unet_40k(nn.Module):\n",
    "    \"\"\"Define the Spherical UNet structure\n",
    "\n",
    "    \"\"\"    \n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        \"\"\" Initialize the Spherical UNet.\n",
    "\n",
    "        Parameters:\n",
    "            in_ch (int) - - input features/channels\n",
    "            out_ch (int) - - output features/channels\n",
    "        \"\"\"\n",
    "        super(Unet_40k, self).__init__()\n",
    "\n",
    "        #neigh_indices_10242, neigh_indices_2562, neigh_indices_642, neigh_indices_162, neigh_indices_42 = Get_indices_order()\n",
    "        #neigh_orders_10242, neigh_orders_2562, neigh_orders_642, neigh_orders_162, neigh_orders_42, neigh_orders_12 = Get_neighs_order()\n",
    "        \n",
    "        neigh_orders = Get_neighs_order()\n",
    "        neigh_orders = neigh_orders[1:]\n",
    "        a, b, upconv_top_index_40962, upconv_down_index_40962, upconv_top_index_10242, upconv_down_index_10242,  upconv_top_index_2562, upconv_down_index_2562,  upconv_top_index_642, upconv_down_index_642, upconv_top_index_162, upconv_down_index_162 = Get_upconv_index() \n",
    "\n",
    "        chs = [in_ch, 32, 64, 128, 256, 512]\n",
    "        \n",
    "        conv_layer = onering_conv_layer\n",
    "\n",
    "        self.down1 = down_block(conv_layer, chs[0], chs[1], neigh_orders[0], None, True)\n",
    "        self.down2 = down_block(conv_layer, chs[1], chs[2], neigh_orders[1], neigh_orders[0])\n",
    "        self.down3 = down_block(conv_layer, chs[2], chs[3], neigh_orders[2], neigh_orders[1])\n",
    "        self.down4 = down_block(conv_layer, chs[3], chs[4], neigh_orders[3], neigh_orders[2])\n",
    "        self.down5 = down_block(conv_layer, chs[4], chs[5], neigh_orders[4], neigh_orders[3])\n",
    "      \n",
    "        self.up1 = up_block(conv_layer, chs[5], chs[4], neigh_orders[3], upconv_top_index_642, upconv_down_index_642)\n",
    "        self.up2 = up_block(conv_layer, chs[4], chs[3], neigh_orders[2], upconv_top_index_2562, upconv_down_index_2562)\n",
    "        self.up3 = up_block(conv_layer, chs[3], chs[2], neigh_orders[1], upconv_top_index_10242, upconv_down_index_10242)\n",
    "        self.up4 = up_block(conv_layer, chs[2], chs[1], neigh_orders[0], upconv_top_index_40962, upconv_down_index_40962)\n",
    "        \n",
    "        self.outc = nn.Sequential(\n",
    "                nn.Linear(chs[1], out_ch)\n",
    "                )\n",
    "                \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x2 = self.down1(x)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x6 = self.down5(x5)\n",
    "        \n",
    "        x = self.up1(x6, x5)\n",
    "        x = self.up2(x, x4)\n",
    "        x = self.up3(x, x3)\n",
    "        x = self.up4(x, x2) # 40962 * 32\n",
    "        \n",
    "        x = self.outc(x) # 40962 * 36\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([100,  25,   0]), 1: array([220,  20,  10]), 2: array([220,  20,  20]), 3: array([220,  60,  20]), 4: array([ 80, 160,  20]), 5: array([25,  5, 25]), 6: array([255, 192,  32]), 7: array([ 25, 100,  40]), 8: array([120,  70,  50]), 9: array([35, 75, 50]), 10: array([ 20, 100,  50]), 11: array([160, 100,  50]), 12: array([120, 100,  60]), 13: array([ 20, 220,  60]), 14: array([ 60, 220,  60]), 15: array([200,  35,  75]), 16: array([100,   0, 100]), 17: array([220,  20, 100]), 18: array([180,  40, 120]), 19: array([ 75,  50, 125]), 20: array([ 80,  20, 140]), 21: array([140,  20, 140]), 22: array([ 20,  30, 140]), 23: array([225, 140, 140]), 24: array([ 20, 180, 140]), 25: array([220, 180, 140]), 26: array([180, 220, 140]), 27: array([125, 100, 160]), 28: array([ 20, 220, 160]), 29: array([ 70,  20, 170]), 30: array([160, 140, 180]), 31: array([150, 150, 200]), 32: array([ 60,  20, 220]), 33: array([220,  60, 220]), 34: array([220, 180, 220]), 35: array([140, 220, 220])}\n",
      "[32 24 28 ... 26 26 26]\n",
      "[14423100  9221140 10542100 ...  9231540  9231540  9231540]\n",
      "[[ 60  20 220]\n",
      " [ 20 180 140]\n",
      " [ 20 220 160]\n",
      " ...\n",
      " [180 220 140]\n",
      " [180 220 140]\n",
      " [180 220 140]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# from model import Unet_40k, Unet_160k\n",
    "from sphericalunet.utils.vtk import read_vtk, write_vtk, resample_label\n",
    "# from sphericalunet.utils.utils import get_par_36_to_fs_vec\n",
    "from sphericalunet.utils.interp_numpy import resampleSphereSurf\n",
    "\n",
    "\n",
    "\n",
    "def get_par_fs_to_36():\n",
    "    \"\"\" Preprocessing for parcellatiion label \"\"\"\n",
    "    file = r'C:\\Users\\DELL\\Desktop\\kaiti\\Spherical_U-Net\\neigh_indices\\template.vtk'\n",
    "    data = read_vtk(file)\n",
    "    par_fs = data['par_fs']\n",
    "    par_fs_label = np.sort(np.unique(par_fs))\n",
    "    par_dic = {}\n",
    "    for i in range(len(par_fs_label)):\n",
    "        par_dic[par_fs_label[i]] = i\n",
    "    return par_dic\n",
    "\n",
    "\n",
    "def get_par_36_to_fs_vec():\n",
    "    \"\"\" Preprocessing for parcellatiion label \"\"\"\n",
    "    file = r'C:\\Users\\DELL\\Desktop\\kaiti\\Spherical_U-Net\\neigh_indices\\template.vtk'\n",
    "    data = read_vtk(file)\n",
    "    par_fs = data['par_fs']\n",
    "    par_fs_vec = data['par_fs_vec']\n",
    "    par_fs_to_36 = get_par_fs_to_36()\n",
    "    par_36_to_fs = dict(zip(par_fs_to_36.values(), par_fs_to_36.keys()))\n",
    "    par_36_to_fs_vec = {}\n",
    "    for i in range(len(par_fs_to_36)):\n",
    "        par_36_to_fs_vec[i] = par_fs_vec[np.where(par_fs == par_36_to_fs[i])[0][0]]\n",
    "    return par_36_to_fs_vec\n",
    "\n",
    "\n",
    "# def get_par_36_to_fs_vec():\n",
    "#     \"\"\" Preprocessing for parcellatiion label \"\"\"\n",
    "#     file = '/media/fenqiang/DATA/unc/Data/NITRC/data/left/train/MNBCP107842_809.lh.SphereSurf.Orig.Resample.vtk'\n",
    "#     data = read_vtk(file)\n",
    "#     par_fs = data['par_fs']\n",
    "#     par_fs_vec = data['par_fs_vec']\n",
    "#     par_fs_to_36 = get_par_fs_to_36()\n",
    "#     par_36_to_fs = dict(zip(par_fs_to_36.values(), par_fs_to_36.keys()))\n",
    "#     par_36_to_fs_vec = {}\n",
    "#     for i in range(len(par_fs_to_36)):\n",
    "#         par_36_to_fs_vec[i] = par_fs_vec[np.where(par_fs == par_36_to_fs[i])[0][0]]\n",
    "#     return par_36_to_fs_vec\n",
    "\n",
    "class BrainSphere(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root1):\n",
    "\n",
    "        self.files = sorted(glob.glob(os.path.join(root1, '*.vtk')))    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file = self.files[index]\n",
    "        data = read_vtk(file)\n",
    "       \n",
    "        return data, file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "def inference(curv, sulc, model):\n",
    "    feats =torch.cat((curv, sulc), 1)\n",
    "    feat_max = [1.2, 13.7]\n",
    "    for i in range(feats.shape[1]):\n",
    "        feats[:,i] = feats[:, i]/feat_max[i]\n",
    "    feats = feats.to(device)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(feats)\n",
    "    pred = prediction.max(1)[1]\n",
    "    pred = pred.cpu().numpy()\n",
    "    return pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    # parser = argparse.ArgumentParser(description='Predict the parcellation maps with 36 regions from the input surfaces',\n",
    "    #                                  formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    # parser.add_argument('--hemisphere', '-hemi', default='left',\n",
    "    #                     choices=['left', 'right'], \n",
    "    #                     help=\"Specify the hemisphere for parcellation, left or right.\")\n",
    "    # parser.add_argument('--level', '-l', default='7',\n",
    "    #                     choices=['7', '8'],\n",
    "    #                     help=\"Specify the level of the surfaces' resolution. Generally, level 7 with 40962 vertices is sufficient, level 8 with 163842 vertices is more accurate but slower.\")\n",
    "    # parser.add_argument('--input', '-i', metavar='INPUT',\n",
    "    #                     help='filename of input surface')\n",
    "    # parser.add_argument('--output', '-o',  default='[input].parc.vtk', metavar='OUTPUT',\n",
    "    #                     help='Filename of ouput surface.')\n",
    "    # parser.add_argument('--device', default='GPU', choices=['GPU', 'CPU'], \n",
    "    #                     help='the device for running the model.')\n",
    "\n",
    "    # args =  parser.parse_args()\n",
    "    in_file = 'C:/Users/DELL/Desktop/kaiti/Spherical_U-Net/examples/left_hemisphere/40962/test1.lh.40k.vtk'\n",
    "    out_file = 'C:/Users/DELL/Desktop/kaiti/Spherical_U-Net/examples/left_hemisphere/40962/test_output.vtk'\n",
    "    hemi = 'left'\n",
    "    level = 7   \n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "    model = Unet_40k(2, 36)\n",
    "    # model_path = '40k_curv_sulc.pkl'\n",
    "    n_vertices = 40962\n",
    "\n",
    "    \n",
    "    model_path = r'C:\\Users\\DELL\\Desktop\\kaiti\\Spherical_U-Net\\trained_models\\left_hemi_40k_curv_sulc.pkl'\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "       \n",
    "    par_fs_to_36 = get_par_fs_to_36()\n",
    "    par_36_to_fs = dict(zip(par_fs_to_36.values(), par_fs_to_36.keys()))\n",
    "    par_36_to_fs_vec = get_par_36_to_fs_vec()\n",
    "    print(par_36_to_fs_vec)\n",
    "\n",
    "    # template = read_vtk('C:/Users/DELL/Desktop/kaiti/SphericalUNetPackage/sphericalunet/utils/neigh_indices/sphere_' + str(n_vertices) + '_rotated_0.vtk')\n",
    "    # if in_file is not None:\n",
    "    #     orig_surf = read_vtk(in_file)\n",
    "    #     curv_temp = orig_surf['curv']\n",
    "    #     if len(curv_temp) != n_vertices:\n",
    "    #         sucu = resampleSphereSurf(orig_surf['vertices'], template['vertices'], \n",
    "    #                                   np.concatenate((orig_surf['sulc'][:,np.newaxis], \n",
    "    #                                                   orig_surf['curv'][:,np.newaxis]),\n",
    "    #                                                  axis=1))\n",
    "    #         sulc = sucu[:,0]\n",
    "    #         curv = sucu[:,1]\n",
    "    #     else:\n",
    "    #          curv = orig_surf['curv'][0:n_vertices]\n",
    "    #          sulc = orig_surf['sulc'][0:n_vertices]\n",
    "        \n",
    "    #     curv = torch.from_numpy(curv).unsqueeze(1) \n",
    "    #     sulc = torch.from_numpy(sulc).unsqueeze(1)\n",
    "        \n",
    "    #     pred = inference(curv, sulc, model)\n",
    "    #     print(pred)\n",
    "    #     # pred = par_36_to_fs_vec[pred]\n",
    "        \n",
    "    #     # orig_lbl = resample_label(template['vertices'], orig_surf['vertices'], pred)\n",
    "        \n",
    "    #     # orig_surf['par_fs_vec'] = orig_lbl\n",
    "    #     # write_vtk(orig_surf, out_file)\n",
    "   \n",
    "    # par_fs_to_36 = get_par_fs_to_36()\n",
    "\n",
    "\n",
    "    if in_file is not None:\n",
    "        data = read_vtk(in_file)\n",
    "        curv_temp = data['curv']\n",
    "        if len(curv_temp) != n_vertices:\n",
    "            raise NotImplementedError('Input surfaces level is not consistent with the level '+ level + ' that the model was trained on.')\n",
    "        curv = torch.from_numpy(data['curv'][0:n_vertices]).unsqueeze(1) # use curv data with 40k vertices\n",
    "        sulc = torch.from_numpy(data['sulc'][0:n_vertices]).unsqueeze(1) # use sulc data with 40k vertices\n",
    "        pred = inference(curv, sulc, model)\n",
    "        print(pred)\n",
    "        data['par_fs'] = np.array([par_36_to_fs[i] for i in pred])\n",
    "        data['par_fs_vec'] = np.array([par_36_to_fs_vec[i] for i in pred])\n",
    "        print(data['par_fs'])\n",
    "        print(data['par_fs_vec'])\n",
    "        write_vtk(data, out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
